{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search = False\n",
    "# load\n",
    "file_name=\"ensemble5\"\n",
    "# file_name=\"Rand_numbers_ln\"\n",
    "# file_name=\"Rand_numbers_ln_m\"\n",
    "# file_name=\"Rand_numbers_ang\"\n",
    "scaler=pd.read_pickle(f'./pickle/{file_name}_scaler.pickle')\n",
    "clf=pd.read_pickle(f'./pickle/{file_name}_clf.pickle')\n",
    "# scaler = pd.read_pickle('./pickle/ensemble3_scaler.pickle')\n",
    "# clf=pd.read_pickle('./pickle/ensemble3_clf.pickle')\n",
    "# scaler = pd.read_pickle('./pickle/Rand_numbers_ang_scaler.pickle')\n",
    "# clf = pd.read_pickle('./pickle/Rand_numbers_ang_clf.pickle')\n",
    "# clf=pd.read_pickle('./pickle/ensemble3_clf.pickle')\n",
    "# scaler = pd.read_pickle('./pickle/ensemble_rf_line_3_scaler.pickle')\n",
    "# clf=pd.read_pickle('./pickle/ensemble_rf_line_3_clf.pickle')\n",
    "# clf2=pd.read_pickle('./pickle/rf_clf.pickle')\n",
    "# clf_vote=VotingClassifier(estimators=[('rf_l3', clf), ('rf', clf2)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "sudoku_026\n",
      "sudoku_027\n",
      "sudoku_028\n",
      "sudoku_029\n",
      "sudoku_030\n"
     ]
    }
   ],
   "source": [
    "import recog_l2\n",
    "# ディレクトリ内のファイルを取得する\n",
    "dir_path = \"./data/level1/\"\n",
    "files = os.listdir(dir_path)\n",
    "# jpgファイルのみを取得する\n",
    "jpg_files = [os.path.splitext(f)[0] for f in files if f.endswith('.jpg')]\n",
    "\n",
    "# ファイル名を表示する\n",
    "for f in jpg_files:\n",
    "    print(f)\n",
    "# jpg_files\n",
    "pixel=60\n",
    "file=\"sample\"\n",
    "problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",arc_epsilon=5e-2)\n",
    "img=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",arc_epsilon=5e-2,ret_img=1,erase_line=0)\n",
    "# plt.imshow(img)\n",
    "# problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_026 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_027 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_028 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_029 0.9753 2.0 failure rate: 0.05\n",
      "sudoku_030 1.0000 0.0 failure rate: 0.00\n"
     ]
    }
   ],
   "source": [
    "for file in jpg_files:\n",
    "        # pixel=60\n",
    "        problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",clf=clf,scaler=scaler,n_close=0,n_open=3,prior_close=1,trim_percentage=0.008,mean_white_axis=0,arc_epsilon=5e-2,erase_line=0,otsu_times=1.22)\n",
    "        with open(f'./{dir_path}{file}.txt', 'r') as f:\n",
    "            matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "        # print matrix data by row\n",
    "        # for row in matrix_data:\n",
    "        #     print(row)\n",
    "        matrix_array = np.array(matrix_data)\n",
    "        validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "        proba=sum(validate)/len(validate)\n",
    "        zeros=np.sum(np.array(matrix_data) == 0)\n",
    "        nonzeros=81-zeros\n",
    "        failed=81-proba*81\n",
    "        print(f\"{file}\",\"{:.4f}\".format(proba),failed,f\"failure rate: {failed/nonzeros:.2f}\")\n",
    "        # probas.append(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "sudoku_00026\n",
      "sudoku_00027\n",
      "sudoku_00028\n",
      "sudoku_00030\n",
      "sudoku_00031\n"
     ]
    }
   ],
   "source": [
    "import recog_l2\n",
    "# ディレクトリ内のファイルを取得する\n",
    "dir_path = \"./data/level2/\"\n",
    "files = os.listdir(dir_path)\n",
    "# jpgファイルのみを取得する\n",
    "jpg_files = [os.path.splitext(f)[0] for f in files if f.endswith('.jpg')]\n",
    "\n",
    "# ファイル名を表示する\n",
    "for f in jpg_files:\n",
    "    print(f)\n",
    "# jpg_files\n",
    "pixel=60\n",
    "file=\"sample\"\n",
    "problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",arc_epsilon=5e-2)\n",
    "img=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",arc_epsilon=5e-2,ret_img=1,erase_line=0)\n",
    "# plt.imshow(img)\n",
    "# problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_00026 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_00027 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_00028 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_00030 1.0000 0.0 failure rate: 0.00\n",
      "sudoku_00031 1.0000 0.0 failure rate: 0.00\n"
     ]
    }
   ],
   "source": [
    "for file in jpg_files:\n",
    "        # pixel=60\n",
    "        problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",clf=clf,scaler=scaler,n_close=0,n_open=3,prior_close=1,trim_percentage=0.008,mean_white_axis=0,arc_epsilon=5e-2,erase_line=0)\n",
    "        with open(f'./{dir_path}{file}.txt', 'r') as f:\n",
    "            matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "        # print matrix data by row\n",
    "        # for row in matrix_data:\n",
    "        #     print(row)\n",
    "        matrix_array = np.array(matrix_data)\n",
    "        validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "        proba=sum(validate)/len(validate)\n",
    "        zeros=np.sum(np.array(matrix_data) == 0)\n",
    "        nonzeros=81-zeros\n",
    "        failed=81-proba*81\n",
    "        print(f\"{file}\",\"{:.4f}\".format(proba),failed,f\"failure rate: {failed/nonzeros:.2f}\")\n",
    "        # probas.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## search param\n",
    "##################################\n",
    "import recog_l2\n",
    "import sys\n",
    "# import cv2\n",
    "if not(param_search):\n",
    "    sys.exit()\n",
    "param_list=[1,3,5,7,9]\n",
    "\n",
    "for param in param_list:\n",
    "    probas=[]\n",
    "    for file in jpg_files:\n",
    "        # pixel=60\n",
    "        problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",clf=clf,scaler=scaler,otsu_times=1.22,sigmaSpace=param,n_close=1,n_open=2,prior_close=1,trim_percentage=0.008,mean_white_axis=0,arc_epsilon=5e-2,erase_line=0)\n",
    "        with open(f'./{dir_path}{file}.txt', 'r') as f:\n",
    "            matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "        # print matrix data by row\n",
    "        # for row in matrix_data:\n",
    "        #     print(row)\n",
    "        # convert the matrix data into a numpy array\n",
    "        matrix_array = np.array(matrix_data)\n",
    "        validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "        proba=sum(validate)/len(validate)\n",
    "        zeros=np.sum(np.array(matrix_data) == 0)\n",
    "        nonzeros=81-zeros\n",
    "        failed=81-proba*81\n",
    "        # print(f\"{file}\",\"{:.4f}\".format(proba),failed,f\"failure rate: {failed/nonzeros:.2f}\")\n",
    "        probas.append(proba)\n",
    "    print(\"param\",param,\"average proba:\",\"{:.4f}\".format(sum(probas)/len(probas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_close 0 n_open 0 average proba: 0.9218\n",
      "n_close 0 n_open 1 average proba: 0.9362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hakos\\Downloads\\advml\\sudoku-solver-hakoshie\\test.ipynb セル 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m probas\u001b[39m=\u001b[39m[]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m jpg_files:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# pixel=60\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     problem\u001b[39m=\u001b[39mrecog_l2\u001b[39m.\u001b[39mrecognize(path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00mdir_path\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m,clf\u001b[39m=\u001b[39mclf,scaler\u001b[39m=\u001b[39mscaler,n_close\u001b[39m=\u001b[39mi,n_open\u001b[39m=\u001b[39mj,prior_close\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,trim_percentage\u001b[39m=\u001b[39m\u001b[39m0.008\u001b[39m,mean_white_axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,arc_epsilon\u001b[39m=\u001b[39m\u001b[39m5e-2\u001b[39m,erase_line\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00mdir_path\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hakos/Downloads/advml/sudoku-solver-hakoshie/test.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         matrix_data \u001b[39m=\u001b[39m [[\u001b[39mint\u001b[39m(num) \u001b[39mfor\u001b[39;00m num \u001b[39min\u001b[39;00m line\u001b[39m.\u001b[39msplit()] \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f]\n",
      "File \u001b[1;32mc:\\Users\\hakos\\Downloads\\advml\\sudoku-solver-hakoshie\\recog_l2.py:296\u001b[0m, in \u001b[0;36mrecognize\u001b[1;34m(path, clf, scaler, pixel, ret_img, n_open, n_close, prior_close, trim_percentage, mean_white_axis, arc_epsilon, erase_line, white_thres, otsu_times, clf_f_name, clf_f, scaler_f, sigmaColor, sigmaSpace)\u001b[0m\n\u001b[0;32m    294\u001b[0m digit_square \u001b[39m=\u001b[39m digit_square\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m255.0\u001b[39m\n\u001b[0;32m    295\u001b[0m digit_square\u001b[39m=\u001b[39mscaler\u001b[39m.\u001b[39mtransform(digit_square)\n\u001b[1;32m--> 296\u001b[0m prediction \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(digit_square)\n\u001b[0;32m    297\u001b[0m predicted_digit \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n\u001b[0;32m    298\u001b[0m predicted_numbers\u001b[39m.\u001b[39mappend(prediction[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:873\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    868\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[0;32m    869\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    870\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m    871\u001b[0m ]\n\u001b[0;32m    872\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[1;32m--> 873\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose, require\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msharedmem\u001b[39m\u001b[39m\"\u001b[39m)(\n\u001b[0;32m    874\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39mpredict_proba, X, all_proba, lock)\n\u001b[0;32m    875\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\n\u001b[0;32m    876\u001b[0m )\n\u001b[0;32m    878\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[0;32m    879\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:650\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[1;34m(predict, X, out, lock)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[0;32m    644\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \n\u001b[0;32m    647\u001b[0m \u001b[39m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 650\u001b[0m     prediction \u001b[39m=\u001b[39m predict(X, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    651\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m    652\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hakos\\miniconda3\\envs\\advml\\Lib\\site-packages\\sklearn\\tree\\_classes.py:923\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    921\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    922\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[1;32m--> 923\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    926\u001b[0m     proba \u001b[39m=\u001b[39m proba[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## search n_close and n_open\n",
    "##################################\n",
    "import recog_l2\n",
    "# import cv2\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        probas=[]\n",
    "        for file in jpg_files:\n",
    "            # pixel=60\n",
    "            problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",clf=clf,scaler=scaler,n_close=i,n_open=j,prior_close=1,trim_percentage=0.008,mean_white_axis=0,arc_epsilon=5e-2,erase_line=0)\n",
    "            with open(f'./{dir_path}{file}.txt', 'r') as f:\n",
    "                matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "            # print matrix data by row\n",
    "            # for row in matrix_data:\n",
    "            #     print(row)\n",
    "            # convert the matrix data into a numpy array\n",
    "            matrix_array = np.array(matrix_data)\n",
    "            validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "            proba=sum(validate)/len(validate)\n",
    "            zeros=np.sum(np.array(matrix_data) == 0)\n",
    "            nonzeros=81-zeros\n",
    "            failed=81-proba*81\n",
    "            # print(f\"{file}\",\"{:.4f}\".format(proba),failed,f\"failure rate: {failed/nonzeros:.2f}\")\n",
    "            probas.append(proba)\n",
    "        print(\"n_close\",i,\"n_open\",j,\"average proba:\",\"{:.4f}\".format(sum(probas)/len(probas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_close 0 n_open 0 average proba: 0.9588\n",
      "n_close 0 n_open 1 average proba: 0.9691\n",
      "n_close 0 n_open 2 average proba: 0.9630\n",
      "n_close 0 n_open 3 average proba: 0.9671\n",
      "n_close 1 n_open 0 average proba: 0.9691\n",
      "n_close 1 n_open 1 average proba: 0.9650\n",
      "n_close 1 n_open 2 average proba: 0.9650\n",
      "n_close 1 n_open 3 average proba: 0.9691\n",
      "n_close 2 n_open 0 average proba: 0.9383\n",
      "n_close 2 n_open 1 average proba: 0.9383\n",
      "n_close 2 n_open 2 average proba: 0.9568\n",
      "n_close 2 n_open 3 average proba: 0.9568\n",
      "n_close 3 n_open 0 average proba: 0.9115\n",
      "n_close 3 n_open 1 average proba: 0.9198\n",
      "n_close 3 n_open 2 average proba: 0.9115\n",
      "n_close 3 n_open 3 average proba: 0.9362\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        probas=[]\n",
    "        for file in jpg_files:\n",
    "            # pixel=60\n",
    "            problem=recog_l2.recognize(path=f\"./{dir_path}{file}.jpg\",n_close=i,n_open=j,prior_close=60\n",
    "            with open(f'./{dir_path}{file}.txt', 'r') as f:\n",
    "                matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "            # print matrix data by row\n",
    "            # for row in matrix_data:\n",
    "            #     print(row)\n",
    "            # convert the matrix data into a numpy array\n",
    "            matrix_array = np.array(matrix_data)\n",
    "            validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "            proba=sum(validate)/len(validate)\n",
    "            zeros=np.sum(np.array(matrix_data) == 0)\n",
    "            nonzeros=81-zeros\n",
    "            failed=81-proba*81\n",
    "            # print(f\"{file}\",\"{:.4f}\".format(proba),failed,f\"failure rate: {failed/nonzeros:.2f}\")\n",
    "            probas.append(proba)\n",
    "        print(\"n_close\",i,\"n_open\",j,\"average proba:\",\"{:.4f}\".format(sum(probas)/len(probas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./data/{file}.txt', 'r') as f:\n",
    "#     matrix_data = [[int(num) for num in line.split()] for line in f]\n",
    "# # print matrix data by row\n",
    "# for row in matrix_data:\n",
    "#     print(row)\n",
    "# # convert the matrix data into a numpy array\n",
    "# matrix_array = np.array(matrix_data)\n",
    "# validate=[matrix_data[i][j]==problem[i][j] for i in range(9) for j in range(9)]\n",
    "# proba=sum(validate)/len(validate)\n",
    "# zeros=np.sum(np.array(matrix_data) == 0)\n",
    "# nonzeros=81-zeros\n",
    "# failed=81-proba*81\n",
    "# print(proba,failed,f\"failure rate: {failed/nonzeros:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
